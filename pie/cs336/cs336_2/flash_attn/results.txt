Entry benchmark.
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 128, d_k: 16, result: 0.10030102778975761
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 128, d_k: 32, result: 0.09941584613427124
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 128, d_k: 64, result: 0.09979477807289533
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 128, d_k: 128, result: 0.09855081759171726
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 256, d_k: 16, result: 0.09934333763276344
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 256, d_k: 32, result: 0.09818253563183212
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 256, d_k: 64, result: 0.0989690178187886
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 256, d_k: 128, result: 0.09978514307188845
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 512, d_k: 16, result: 0.09769243513005312
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 512, d_k: 32, result: 0.09974392591535816
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 512, d_k: 64, result: 0.09999840944907214
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 512, d_k: 128, result: 0.09909266967917137
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 1024, d_k: 16, result: 0.09815279739794511
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 1024, d_k: 32, result: 0.09923616173030184
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 1024, d_k: 64, result: 0.09980304627317538
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 1024, d_k: 128, result: 0.09829740272827861
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 2048, d_k: 16, result: 0.12046175447637981
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 2048, d_k: 32, result: 0.11937584152674992
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 2048, d_k: 64, result: 0.1194033408549917
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 2048, d_k: 128, result: 0.12085347587203599
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 4096, d_k: 16, result: 0.3393864279426654
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 4096, d_k: 32, result: 0.33880208606866513
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 4096, d_k: 64, result: 0.3400215830801092
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 4096, d_k: 128, result: 0.34108068727056323
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 8192, d_k: 16, result: 1.2668046829104012
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 8192, d_k: 32, result: 1.2656371823483301
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 8192, d_k: 64, result: 1.2658415746605378
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 8192, d_k: 128, result: 1.2719296407777467
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 16384, d_k: 16, result: 4.084845328837845
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 16384, d_k: 32, result: 4.089826468880295
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 16384, d_k: 64, result: 4.096708550289859
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 16384, d_k: 128, result: 4.116000236783709
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 32768, d_k: 16, result: 15.785127109951443
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 32768, d_k: 32, result: 15.80188179470966
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 32768, d_k: 64, result: 15.807795859671927
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 32768, d_k: 128, result: 15.856686551034736
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 65536, d_k: 16, result: 63.87026243943434
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 65536, d_k: 32, result: 63.95642865009797
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 65536, d_k: 64, result: 64.2075675226027
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float16, seq_len: 65536, d_k: 128, result: 64.52855305857473
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 128, d_k: 16, result: 0.09811830196923907
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 128, d_k: 32, result: 0.09877125874755333
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 128, d_k: 64, result: 0.09790498113396663
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 128, d_k: 128, result: 0.10110501594466911
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 256, d_k: 16, result: 0.10147533311857594
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 256, d_k: 32, result: 0.1026865416755721
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 256, d_k: 64, result: 0.10388480488471019
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 256, d_k: 128, result: 0.10253051346608864
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 512, d_k: 16, result: 0.10295700938068904
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 512, d_k: 32, result: 0.10128225288408772
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 512, d_k: 64, result: 0.10215564802123407
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 512, d_k: 128, result: 0.103863675566271
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 1024, d_k: 16, result: 0.10199448003169904
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 1024, d_k: 32, result: 0.10198594299804563
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 1024, d_k: 64, result: 0.10151724241941934
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 1024, d_k: 128, result: 0.10355087349262668
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 2048, d_k: 16, result: 0.160020684472117
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 2048, d_k: 32, result: 0.1614757187820509
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 2048, d_k: 64, result: 0.1767591599224036
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 2048, d_k: 128, result: 0.19276559346961758
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 4096, d_k: 16, result: 0.49401735296285193
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 4096, d_k: 32, result: 0.5072672706007041
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 4096, d_k: 64, result: 0.5469704843941753
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 4096, d_k: 128, result: 0.6386522922366846
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 8192, d_k: 16, result: 1.8112674440398702
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 8192, d_k: 32, result: 1.8463920182375997
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 8192, d_k: 64, result: 1.9931182935555578
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 8192, d_k: 128, result: 2.3361669874918487
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 16384, d_k: 16, result: 6.378961405088735
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 16384, d_k: 32, result: 6.536436200220801
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 16384, d_k: 64, result: 7.2745238149867335
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 16384, d_k: 128, result: 8.524327016328165
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 32768, d_k: 16, result: 26.764640428686654
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 32768, d_k: 32, result: 27.38390708231664
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 32768, d_k: 64, result: 29.971544604703606
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 32768, d_k: 128, result: 35.694376545567664
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 65536, d_k: 16, result: 113.48186194759676
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 65536, d_k: 32, result: 116.04940024087595
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 65536, d_k: 64, result: 122.24388480480806
kernel: <bound method Function.apply of <class 'native_attention.flashattnTorch'>>, acc: torch.float32, seq_len: 65536, d_k: 128, result: 140.70926012311662
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 128, d_k: 16, result: 0.008573697723247366
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 128, d_k: 32, result: 0.009878249831300237
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 128, d_k: 64, result: 0.009993036193982514
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 128, d_k: 128, result: 0.01120837430270368
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 256, d_k: 16, result: 0.01143259663618051
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 256, d_k: 32, result: 0.013339816994237344
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 256, d_k: 64, result: 0.014003483949644489
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 256, d_k: 128, result: 0.015735256866093485
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 512, d_k: 16, result: 0.016342196146635478
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 512, d_k: 32, result: 0.020547052663974736
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 512, d_k: 64, result: 0.021811739909447965
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 512, d_k: 128, result: 0.025233509539785072
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 1024, d_k: 16, result: 0.026968252603342763
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 1024, d_k: 32, result: 0.03478857551684949
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 1024, d_k: 64, result: 0.03750677092823953
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 1024, d_k: 128, result: 0.04414027616735614
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 2048, d_k: 16, result: 0.04664611011712013
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 2048, d_k: 32, result: 0.06306870464206028
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 2048, d_k: 64, result: 0.06826186671695908
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 2048, d_k: 128, result: 0.08098671289615669
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 4096, d_k: 16, result: 0.09090541965543159
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 4096, d_k: 32, result: 0.1288590654098494
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 4096, d_k: 64, result: 0.14519787638807727
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 4096, d_k: 128, result: 0.18681737899215453
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 8192, d_k: 16, result: 0.29275699020282625
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 8192, d_k: 32, result: 0.3451308218739283
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 8192, d_k: 64, result: 0.3995063730826937
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 8192, d_k: 128, result: 0.5405032281452014
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 16384, d_k: 16, result: 1.090808080629864
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 16384, d_k: 32, result: 1.2527764790517124
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 16384, d_k: 64, result: 1.467818708767532
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 16384, d_k: 128, result: 2.2772717497294153
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 32768, d_k: 16, result: 4.195638743369547
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 32768, d_k: 32, result: 4.890054915319035
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 32768, d_k: 64, result: 5.784986566954444
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 32768, d_k: 128, result: 8.251505819956462
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 65536, d_k: 16, result: 16.570003728866578
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 65536, d_k: 32, result: 19.422628238797188
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 65536, d_k: 64, result: 22.82331772025572
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float16, seq_len: 65536, d_k: 128, result: 31.53717075750089
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 128, d_k: 16, result: 0.009233252055799163
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 128, d_k: 32, result: 0.010325467843628389
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 128, d_k: 64, result: 0.011651971542381657
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 128, d_k: 128, result: 0.01506155730149246
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 256, d_k: 16, result: 0.011738115437215526
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 256, d_k: 32, result: 0.01449494120855654
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 256, d_k: 64, result: 0.01729398806189726
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 256, d_k: 128, result: 0.023884307328308783
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 512, d_k: 16, result: 0.01815857708898265
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 512, d_k: 32, result: 0.023313471492900445
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 512, d_k: 64, result: 0.027982239174068287
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 512, d_k: 128, result: 0.04065342160929439
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 1024, d_k: 16, result: 0.02927597421251248
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 1024, d_k: 32, result: 0.04040469450978445
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 1024, d_k: 64, result: 0.049357415156866584
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 1024, d_k: 128, result: 0.072197478729143
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 2048, d_k: 16, result: 0.05468275483160724
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 2048, d_k: 32, result: 0.07340547981969589
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 2048, d_k: 64, result: 0.0916642282831272
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 2048, d_k: 128, result: 0.13158724587474938
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 4096, d_k: 16, result: 0.11481536993352841
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 4096, d_k: 32, result: 0.1603979660671167
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 4096, d_k: 64, result: 0.20990382024318466
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 4096, d_k: 128, result: 0.32609433728701936
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 8192, d_k: 16, result: 0.38864145350986434
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 8192, d_k: 32, result: 0.4378116677422771
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 8192, d_k: 64, result: 0.5971993929906133
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 8192, d_k: 128, result: 0.9104395044258967
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 16384, d_k: 16, result: 1.4792765842554005
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 16384, d_k: 32, result: 1.6030027894698842
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 16384, d_k: 64, result: 2.478952577891743
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 16384, d_k: 128, result: 3.5961310133677884
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 32768, d_k: 16, result: 5.7364447974492565
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 32768, d_k: 32, result: 6.2773750632167165
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 32768, d_k: 64, result: 8.859339368577933
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 32768, d_k: 128, result: 14.282703766877624
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 65536, d_k: 16, result: 22.612376633557407
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 65536, d_k: 32, result: 24.744566760846038
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 65536, d_k: 64, result: 34.02313971926328
kernel: <bound method Function.apply of <class 'flash_attention.flashattnTriton'>>, acc: torch.float32, seq_len: 65536, d_k: 128, result: 57.066076136997765
